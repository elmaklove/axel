{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8

# LINEAR CLASSIFICATION AND STOCHASTIC GRADIENT DESCENT by AXEL


#Checking the python version
import sys
print ('python: {}'.format(sys.version))

#Loading the libraries
import numpy
from numpy import random
import matplotlib.pyplot as plt
from sklearn.externals.joblib import Memory
from sklearn.datasets import load_svmlight_file
from sklearn.model_selection import train_test_split



# Loading the experiment dataset
#defining thefunction that get the data
def get_data():
    data = load_svmlight_file("https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/australian_scale")    
    return data




#Dividing the data into training and validating set
X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=0.4, random_state=43)

N,D=x_train.shape 
C=len(list(set(y_train))) 


#defining the SVM function and,Initializing SVM model parameters
def svm(w, Xtrain, Ytrain, Xtest, Ytest, Reg):

  gW = numpy.zeros(w.shape) 
  num_classes = w.shape[1]

  scores_train = Xtrain.dot(w) 
  train_loss = 0
  num_train = Xtrain.shape[0]

  margins_train = scores_train - scores_train_correct + 1.0 
  margins_train[numpy.arange(num_train), Ytrain] = 0.0 
  margins_train[margins_train <= 0] = 0.0
  margins_train[margins_train > 0] = 1.0

  Scores_train_correct = scores_train[numpy.arange(num_train), Ytrain]
  Scores_train_correct = numpy.reshape(scores_train_correct, (num_train, 1))
  
  train_loss += numpy.sum(margins_train) / num_train
  train_loss += 0.5 * Reg * numpy.sum(w * w) 
                         
  row_sum = numpy.sum(margins_train, axis=1)                 
  margins_train[numpy.arange(num_train), Ytrain] = -row_sum        
  gW += numpy.dot(Xtrain.T, margins_train)/num_train + Reg * w
    
  test_loss = 0
  Scores_test = Xtest.dot(w) 
  num_test = Xtest.shape[0]

  Scores_test_correct = Scores_test[numpy.arange(num_test), Ytest] 
  Scores_test_correct = numpy.reshape(Scores_test_correct, (num_test, 1)) 
  margins_test = Scores_test - Scores_test_correct + 1.0 
  margins_test[numpy.arange(num_test), Ytest] = 0.0 
  margins_test[margins_test <= 0] = 0.0

  test_loss += numpy.sum(margins_test) / num_test
  test_loss += 0.5 * reg * numpy.sum(w * w)

  return train_loss, test_loss, gW

data = get_data()
X=data[0].toarray()
Y=data[1]
Y=Y.reshape(len(Y),order='C') 
Y=Y.astype(numpy.int) 


w = random.random(size=(D, C)) 
maxIters=500
th = 0 
eta = 0.001 
L_train=[];
L_test=[];


 
for t in range(maxIters):
    Y_train_pred = numpy.dot(x_train,W)
    Y_train_pred[y_train_pred> th] = 1
    Y_train_pred[y_train_pred<=th] = 0
    
    Y_test_pred = numpy.dot(x_test,W)
    Y_test_pred[y_test_pred> th] = 1
    Y_test_pred[y_test_pred<=th] = 0   
    
    train_loss, test_loss, grad_W= svm(w, X_train, Y_train,  X_test, Y_test, Reg= 0.1)
    
    L_train.append (train_loss)
    L_test.append (test_loss)

    while True:

      batch = next_training_batch(data, 123)
      grad_W = evaluate_gradient(loss, batch, W)
      W += -adam * grad_W
      W += -NAG * grad_W
      W += -RMSProp * grad_W
      W += -AdaDelta * grad_W



plt.plot(L_train,'r',label='train loss')
plt.plot(L_test,'b',label='test loss')
# We give the plot a title
plt.title('Loss Curve') 
plt.legend()
plt.show()


"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
