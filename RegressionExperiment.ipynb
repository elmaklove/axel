{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "
# lOGISTIC  REGRESSION AND STOCHASTIC GRADIENT DESCENT by AXEL

#Checking the python version
import sys
print ('python: {}'.format(sys.version))

#Loading the  libraries

import io
import os.path
import torch

import matplotlib.pyplot as plt

import numpy as np
import scipy.sparse as sp
from sklearn.externals.joblib  import Memory

from sklearn.datasets import load_svmlight_file

from sklearn.model_selection import train_test_split
#Loading the experiment data using load_svmlight_file function
#defining the function that get the data
def get_data():
    
data = load_svmlight_file("https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a9a",n_features=123) 
   
    return data[0], data[1]

x,Y = get_data()
x = data[0].toarray()
Y = data[1]

#Dividing the dataset

x_train, x_validation, Y_train, Y_validation = train_test_split(x, Y, test_size=0.8, random_state=93)

# S is batch size; D_in is input dimension;D_out is output dimension.

S,D_in=x_train.shape

D_out=1

#We choose loss function and derivation
 
def loss_function(x, Y, weight, bias, Iterations=500):

    N = Float(len(Y))

    total_error = 0.0

    for i in range(Iterations):

        total_error += (X[i] - (weight*y[i] + bias))**2

    return total_error/N 



models = []

models.append(('LR', LogisticRegression()))

def logistic_regression(X, y, m=0, b=0, epochs=100, learning_rate=0.0006):

     N = float(len(y))

     for i in range(epochs):

          y_prec = (m * X) + b

          loss = sum([data**2 for data in (y-y_prec)]) / N

          loss = ((y_prec - y) ** 2)

          print ("loss:", loss)
  
     return loss

# Compute and print loss
   
 L_train.append ( (Y_train_pred - Y_train).pow(2).sum())
     L_validation.append ( (Y_validation_pred-Y_validation).pow(2).sum())


# We update weights using gradient descent
def sigmoid_activaion():
#compute and return the sigmoig
#given the input value    
    return 1.0/(1 + np.exp(-X))

def next_batch(X,Y,S):
   for i in np.arange(0, X.shape[0], S)
   yield (X[i:i + S], Y[i:i + S])
   
while True:

      batch = next_training_batch(data, 123)
      grad_W = evaluate_gradient(loss, batch, W)
      W += -adam * grad_W
      W += -NAG * grad_W
      W += -RMSProp * grad_W
      W += -AdaDelta * grad_W


plt.plot(L_train,'r',label='Ltrain')

plt.plot(L_validation,'b',label='Lvalidation')

plt.legend()

plt.show()

"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
