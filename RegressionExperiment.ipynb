{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "
# LINEAR REGRESSION AND GRADIENT DESCENT by AXEL


#Checking the python version
import sys
print ('python: {}'.format(sys.version))

#Loading the  libraries

import io
import os.path
import torch

import matplotlib.pyplot as plt

import numpy as np
import scipy.sparse as sp
from sklearn.externals.joblib  import Memory

from sklearn.datasets import load_svmlight_file

from sklearn.model_selection import train_test_split







#Loading the experiment data using load_svmlight_file function
#defining the function that get the data
def get_data():
    
data = load_svmlight_file("https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/housing_scale",n_features=13) 
   
    return data

[0], data[1]

x,Y = get_data()
x = data[0].toarray()
Y = data[1]







#Dividing the dataset

x_train, x_validation, Y_train, Y_validation = train_test_split(x, Y, test_size=0.5, random_state=43)


# S is batch size; D_in is input dimension;D_out is output dimension.

S,D_in=x_train.shape

D_out=1







# We create random input and output data

Dtype = torch.DoubleTensor


# We randomly initialize weights

W = torch.randn(D_in, D_out).type(Dtype)

b = 0

x_train=torch.from_numpy(x_train)

Y_train=torch.from_numpy(Y_train)

x_validation=torch.from_numpy(x_validation)

Y_validation=torch.from_numpy(Y_validation)



#We choose loss function and derivation
def linear_regression(x, Y, weight, bias):

    Y_prec = (weight*x)+ bias

    print (Y_prec)

    return Y_prec  
 
def loss_function(x, Y, weight, bias, Iterations=500):

    N = Float(len(Y))

    total_error = 0.0

    for i in range(Iterations):

        total_error += (X[i] - (weight*y[i] + bias))**2

    return total_error/N 


def linear_regression(X, y, m=0, b=0, epochs=100, learning_rate=0.0001):

     N = float(len(y))

     for i in range(epochs):

          y_prec = (m * X) + b

          loss = sum([data**2 for data in (y-y_prec)]) / N

          loss = ((y_prec - y) ** 2)

          print ("loss:", loss)
  
     return loss          

eta = 1e-6

L_train=[];

L_validation=[];

for t in range(400000):
   

 # Compute predicted y
 and Performs a matrix multiplication of the matrices mat1 and mat2.
    
 Y_train_pred = torch.add(x_train.mm(W),b) 
 Y_validation_pred =torch.add( x_validation.mm(W),b) 


   
 # Compute and print loss
   
 L_train.append ( (Y_train_pred - Y_train).pow(2).sum())
   
 L_validation.append ( (Y_validation_pred - Y_validation).pow(2).sum())
  
 
  
 # Backprop to compute gradients of w1 and w2 with respect to loss
  
  grad_Y_train_pred = 2.0 * (Y_train_pred - Y_train)
  
  grad_W = 1/N * x_train.t().mm(grad_Y_train_pred)
   
 grad_b = 1/N * grad_Y_train_pred.sum()
 
   
  
  # We update weights using gradient descent
  
  W -= eta * grad_w
  
  b -= eta * grad_b





plt.plot(L_train,'r',label='Ltrain')

plt.plot(L_validation,'b',label='Lvalidation')

plt.legend()

plt.show()











"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
